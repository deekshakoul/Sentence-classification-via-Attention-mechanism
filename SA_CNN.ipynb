{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SA_CNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/deekshakoul/Sentence-classification-via-Attention-mechanism/blob/main/SA_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HydDdhMJqJCq"
      },
      "source": [
        "Model leverages the power of the self-attention mechanism and CNN for sentence classification on multiple datasets. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGwonHTieX_e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "65db17ae-9363-43e1-f433-6520c5955e21"
      },
      "source": [
        "!git clone https://github.com/deekshakoul/sent-conv-torch.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'sent-conv-torch'...\n",
            "remote: Enumerating objects: 477, done.\u001b[K\n",
            "remote: Total 477 (delta 0), reused 0 (delta 0), pack-reused 477\u001b[K\n",
            "Receiving objects: 100% (477/477), 58.85 MiB | 13.17 MiB/s, done.\n",
            "Resolving deltas: 100% (309/309), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NUxO8vFopa0C",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "a1556d15-69e1-428b-ca78-d485d553325a"
      },
      "source": [
        "import re,string\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import math\n",
        "import os\n",
        "import copy\n",
        "#=========================pytorch========================#\n",
        "import torch   \n",
        "from torchtext import data ,vocab\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "SEED = 134\n",
        "torch.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True  \n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n",
        "print(\"cuda? \",torch.cuda.is_available())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "cuda?  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cAJaFUA60E5I"
      },
      "source": [
        "def clean_str(string):\n",
        "  string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)     \n",
        "  string = re.sub(r\"\\'s\", \" \\'s\", string) \n",
        "  string = re.sub(r\"\\'ve\", \" \\'ve\", string) \n",
        "  string = re.sub(r\"n\\'t\", \" n\\'t\", string) \n",
        "  string = re.sub(r\"\\'re\", \" \\'re\", string) \n",
        "  string = re.sub(r\"\\'d\", \" \\'d\", string) \n",
        "  string = re.sub(r\"\\'ll\", \" \\'ll\", string) \n",
        "  string = re.sub(r\",\", \" , \", string) \n",
        "  string = re.sub(r\"!\", \" ! \", string) \n",
        "  string = re.sub(r\"\\(\", \" ( \", string) \n",
        "  string = re.sub(r\"\\)\", \" ) \", string) \n",
        "  string = re.sub(r\"\\?\", \" ? \", string) \n",
        "  string = re.sub(r\"\\s{2,}\", \" \", string)    \n",
        "  return string.strip().lower()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VjPRYm6WpfTd"
      },
      "source": [
        "from pre_process import create_alldata_csv\n",
        "df = create_alldata_csv('subj.all') \n",
        "#subj.all , custrev.all , rt-polarity.all , mpqa.all\n",
        "df=df.sample(frac=1)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNC3xMfsxQjj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "0a801431-30f9-4177-8e1f-e0528b8490eb"
      },
      "source": [
        "######################Dont run again and again###########################\n",
        "!cp /content/drive/My\\ Drive/iisc/summer/datasets/glove.840B.300d.zip /content\n",
        "!unzip /content/glove.840B.300d.zip\n",
        "import os\n",
        "emb_path = '/content'\n",
        "vec = vocab.Vectors(os.path.join(emb_path, 'glove.840B.300d.txt'), cache=emb_path,unk_init = torch.Tensor.normal_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  /content/glove.840B.300d.zip\n",
            "  inflating: glove.840B.300d.txt     \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 2195956/2196017 [05:10<00:00, 7613.32it/s]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGC80vcExNAS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "outputId": "61e0069c-a2e6-474a-8f41-3ba854123d91"
      },
      "source": [
        "'''\n",
        "  Take data from fulldata.csv\n",
        "  Build_vocab: give indexing to words(tokens)\n",
        "  <UNK>:0 and <PAD>:1\n",
        "  Using vec(glove), create embeddings given the token \n",
        "'''\n",
        "TEXT = data.Field(lower=True,batch_first=True,include_lengths=True)#,stop_words=stop)tokenize='spacy',\n",
        "LABEL = data.LabelField(dtype = torch.int,batch_first=True)\n",
        "fields = [('text',TEXT),('label', LABEL)]\n",
        "all_data = data.TabularDataset(skip_header = True,path='/content/fulldata.csv',format = 'csv', fields=fields)\n",
        "TEXT.build_vocab(all_data,vectors = vec ) #all_data has tokens(words) and they will be mapped to embeddings of glove..if word is in glove\n",
        "LABEL.build_vocab(all_data)\n",
        "word_embeddings = TEXT.vocab.vectors\n",
        "word_embeddings[1] = torch.zeros(word_embeddings.shape[1])\n",
        "print(\"Size of TEXT vocabulary:\",len(TEXT.vocab)) #total words\n",
        "print(\"===========\\n\",TEXT.vocab.vectors)\n",
        "#u will notice <UNK>:0 and <PAD>:1 initialized randomly.....rest words or indices will always remain same"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of TEXT vocabulary: 21324\n",
            "===========\n",
            " tensor([[ 0.1040,  0.8886,  1.7246,  ...,  0.2860, -0.0557, -1.4377],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0828,  0.6720, -0.1499,  ..., -0.1918, -0.3785, -0.0659],\n",
            "        ...,\n",
            "        [ 0.8111,  0.3765,  0.3113,  ...,  0.3401, -0.1715,  0.0132],\n",
            "        [-0.1490, -0.8729, -0.2281,  ..., -0.0667, -0.3832,  0.3249],\n",
            "        [ 0.6228, -0.0159,  0.3422,  ...,  0.2939, -0.1054, -0.8087]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFy-KSOPzZAu"
      },
      "source": [
        "###FOLDS GENERATION#####\n",
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "cv = 4\n",
        "kf = StratifiedKFold(cv, True, SEED)\n",
        "\n",
        "folds=[]\n",
        "for tr, tt in kf.split(df[\"text\"], df[\"label\"]):\n",
        "    #print(train_index, test_index)\n",
        "    #print(len(train_index), len(test_index))\n",
        "    folds.append((list(tr), list(tt)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8NPOoy432Ec"
      },
      "source": [
        "def build_criterion(): \n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  criterion = criterion.to(device)\n",
        "  return criterion\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "#GLOBAL PARAMS#\n",
        "global testing\n",
        "testing =  False\n",
        "SRC_PAD_IDX=TEXT.vocab.stoi[TEXT.pad_token]\n",
        "d_model = 300\n",
        "d_v = 128\n",
        "d_k = 128\n",
        "n_heads = 4\n",
        "batch = 16\n",
        "output_size = 2\n",
        "EPOCHS= 200\n",
        "clip=1e-3\n",
        "# lrate = 1e-4\n",
        "# wd=3e-5\n",
        "#GOOD STILL\n",
        "lrate = 1.5e-5\n",
        "wd=3e-5\n",
        "#lrate = 1e-5\n",
        "#wd=1e-4\n",
        "# dropouts#\n",
        "d1=0.5#dropout to main network\n",
        "d2=0.5\n",
        "d3=0.5#scaled dot product\n",
        "d4=0.5#positional-encoding\n",
        "#CNN PARAMS#\n",
        "Ci= 1\n",
        "Co=100\n",
        "Ks=[3,4,5]\n",
        "max_kernel_size = 5\n",
        "\n",
        "best_acc_overall =0\n",
        "\n",
        "for i, (tr_ix, tt_ix) in enumerate(folds):\n",
        "  print(\"Fold ==> \",i+1)\n",
        "  train_data = [item for i,item in enumerate(all_data) if i in tr_ix]\n",
        "  test_data = [item for i,item in enumerate(all_data) if i in tt_ix]\n",
        "  train = data.Dataset(train_data,fields)\n",
        "  test = data.Dataset(test_data,fields)\n",
        "  print(\"train size: \", len(train_data), \" and test size: \", len(test_data))\n",
        "  train_itr,test_itr = data.BucketIterator.splits(\n",
        "                (train, test), \n",
        "                batch_size = batch,\n",
        "                sort_key = lambda x: len(x.text),\n",
        "                sort_within_batch=True,\n",
        "                device = device,\n",
        "                shuffle=True\n",
        "              )   \n",
        "  \n",
        "  model = MainNetwork()\n",
        "  model = model.to(device)\n",
        "  print(f'The model has {count_parameters(model):,} trainable parameters')\n",
        "  criterion = build_criterion()\n",
        "  optimizer = torch.optim.Adam(model.parameters(),lr=lrate, weight_decay=wd)\n",
        " \n",
        "  best_acc = 0.0\n",
        "  best_loss = 100.0\n",
        "  test_loss = []\n",
        "  tr_loss = []\n",
        "  test_acc = []\n",
        "  tr_acc = []  \n",
        " \n",
        "  for ep in range(EPOCHS):\n",
        "    train_loss, train_acc = training(model,train_itr,criterion,batch, optimizer,clip=clip)\n",
        "    eval_loss, eval_acc = evaluation(model,test_itr,batch)\n",
        "    print(f'Epoch: {ep+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f} %, Test Loss: {eval_loss:3f}, test Acc: {eval_acc:.2f}%')    \n",
        " \n",
        "    test_loss.append(eval_loss)\n",
        "    tr_loss.append(train_loss)\n",
        "    test_acc.append(eval_acc)\n",
        "    tr_acc.append(train_acc)\n",
        "\n",
        "    if eval_acc > best_acc:\n",
        "      best_acc = eval_acc\n",
        "      \n",
        "    if eval_loss < best_loss:\n",
        "      best_loss = eval_loss\n",
        "\n",
        "  if  best_acc_overall < best_acc:  #best_acc is for every fold\n",
        "    best_acc_overall = best_acc\n",
        "    model_copy = copy.deepcopy(model)\n",
        "    print(\"For fold \",i+1,\" given \",cv,\" folds\", \" best accuracy for this fold is  \",best_acc)\n",
        "\n",
        "\n",
        "  plot_losses_per_fold(tr_loss,test_loss,i+1)\n",
        "  plot_losses_per_fold(tr_acc,test_acc,i+1,\"ACC\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WpmezL6_LaDl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "ec610e8f-c1d0-42b4-c1a3-770d9ee89e3d"
      },
      "source": [
        "best_acc_overall"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "81.35593220338983"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dFF2EC-qAju"
      },
      "source": [
        "def plot_losses_per_fold(tr, tt,fold,c=\"LOSS\"):\n",
        "  epoch_count = range(1, len(tr) + 1)\n",
        "  name = 'MR' + \"_CV\" + str(fold)  + \"_\" + c\n",
        "  plt.plot(epoch_count, tr, 'r--')\n",
        "  plt.plot(epoch_count, tt, 'b-')\n",
        "  if c == \"LOSS\":\n",
        "    plt.legend(['Training Loss', 'Test Loss'])\n",
        "  else:\n",
        "    plt.legend(['Training Accuracy', 'Test Accuracy'])\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.ylabel('Loss') \n",
        "  if c==\"ACC\":\n",
        "    plt.ylabel('Accuracy') \n",
        "  \n",
        "  plt.xticks(np.arange(min(epoch_count), max(epoch_count)+1, 40.0))\n",
        "  # plt.ylim(-5, 5)\n",
        "  plt.savefig(name)\n",
        "  plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlLbPXi17b4g"
      },
      "source": [
        "#--------------------------------------******TESTING PURPOSES*******------------------------------------#\n",
        "dd = TEXT.vocab.stoi\n",
        "k = list(dd.keys())\n",
        "v = list(dd.values())\n",
        "def get_sent(text):\n",
        "  sent=[]  \n",
        "  for i in text:\n",
        "    sent.append(k[v.index(i)])\n",
        "  return sent\n",
        "def get_indices_sent(text):\n",
        "  ind=[]\n",
        "  tokens = text.split()\n",
        "  for i in range(len(tokens)):  \n",
        "    if tokens[i] in k:\n",
        "      ind.append(dd[tokens[i]])\n",
        "    else:\n",
        "      ind.append(TEXT.vocab.stoi[TEXT.unk_token])\n",
        "  return ind      \n",
        "def make_batch(sent):\n",
        "  l=[]\n",
        "  m=[]\n",
        "  max = 0\n",
        "  for i in range(len(sent)):\n",
        "    ind = get_indices_sent(clean_str(sent[i]))\n",
        "    l.append(ind)\n",
        "    if len(ind)>max:\n",
        "      max=len(ind)\n",
        "    m.append(len(ind))\n",
        "\n",
        "  for i in range(len(l)):\n",
        "    length_sent = len(l[i])\n",
        "    if length_sent != max:\n",
        "      l[i].extend([TEXT.vocab.stoi[TEXT.pad_token]]*(max - length_sent))\n",
        "  return torch.LongTensor(l), torch.LongTensor(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6C-ImqTL0TR5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "dedb0fdd-bbe7-46c1-a0d6-18196633ea84"
      },
      "source": [
        "clean_str(sentences[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"this movie does n't deserve the energy it takes to describe how bad it is\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Y8gR3f6DxlS"
      },
      "source": [
        "vocab_dim=len(dd)\n",
        "embed_dim = d_model\n",
        "model = MainNetwork()\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wFKD8tWzP2Du"
      },
      "source": [
        "global attn_wts\n",
        "testing = True\n",
        "\n",
        "sentences =[\"great quality picture and features\"] #CR\n",
        "\n",
        "txt, txt_len = make_batch(sentences)\n",
        "sts = len(sentences)\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  if torch.cuda.is_available():\n",
        "    txt = txt.cuda()\n",
        "    txt_len = txt_len.cuda()\n",
        "  predictions = model(txt)\n",
        "  print(predictions)\n",
        "  print(torch.max(predictions, 1)[1].view(sts).data,\"\\n \\n\")\n",
        "  print(\"================\\n\",attn_wts) #[batch_size x n_heads x words x words ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgoQsFDFSzer",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "bcabeb03-7f18-4798-ef5e-b9f6f02b5b00"
      },
      "source": [
        "xx =clean_str(sentences[0]).split()\n",
        "xx"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'customer', 'support', 'is', 'pathetic']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R1lBFPaI0yKL"
      },
      "source": [
        "sns.heatmap(attn_wts[0][3].cpu(),xticklabels=xx, yticklabels=xx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBU0VCgBQJ_M"
      },
      "source": [
        "class MainNetwork(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MainNetwork,self).__init__(); \n",
        "    self.embedding = nn.Embedding.from_pretrained(word_embeddings,padding_idx= SRC_PAD_IDX,freeze=True) #freeze=True: no learnable params\n",
        "    self.pos_embedding = PositionalEncoding(d_model)\n",
        "    self.self_attn = MultiHeadAttention()\n",
        "\n",
        "    self.layer_norm = nn.LayerNorm(d_model)#, eps=1e-6)\n",
        "    self.dropout = nn.Dropout(p=d1)\n",
        "\n",
        "    ##--------CNN LAYER to get final output--------------##\n",
        "    self.convs = nn.ModuleList([nn.Conv2d(Ci, Co, (K, d_model)) for K in Ks])\n",
        "    self.fc1 = nn.Linear(len(Ks) * Co, output_size)\n",
        "    \n",
        "      \n",
        "  def forward(self,text):    \n",
        "    embeds = self.pos_embedding(self.embedding(text)* math.sqrt(d_model)) \n",
        "    #embeds = self.dropout(embeds)\n",
        "    \n",
        "    text_mask = get_attn_pad_mask(text,text) # [batch, seq_len , seq_len] --> check for some problems\n",
        "    \n",
        "    out,attn =  self.self_attn(embeds, embeds, embeds, text_mask)  #[batch, seq_len, d_model] , [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "    out =  self.layer_norm(out + embeds)\n",
        "   \n",
        "    ##------------CNN Layer---------------##\n",
        "    if out.size()[1] < max_kernel_size:\n",
        "      m = nn.ZeroPad2d((0, 0, 0, max_kernel_size-out.size()[1]))\n",
        "      out = m(out)\n",
        "      \n",
        "    out = out.unsqueeze(1)  # (N, Ci, W, D)\n",
        "    out  = [F.relu(conv(out)).squeeze(3) for conv in self.convs]  # [(N, Co, W), ...]*len(Ks)\n",
        "    out = [F.max_pool1d(i, i.size(2)).squeeze(2) for i in out]  # [(N, Co), ...]*len(Ks)\n",
        "    out = torch.cat(out, 1)\n",
        "    out = self.dropout(out) \n",
        "    logit = self.fc1(out)  # (N, C)\n",
        "\n",
        "    global testing,attn_wts\n",
        "    if testing:\n",
        "      attn_wts = attn\n",
        "\n",
        "    return logit  \n",
        "\n",
        "def get_attn_pad_mask(seq_q, seq_k): #SEE EXPLANATION#1\n",
        "  assert seq_q.dim() == 2 and seq_k.dim() == 2\n",
        "  b_size, len_q = seq_q.size()\n",
        "  b_size, len_k = seq_k.size()\n",
        "  pad_attn_mask = seq_k.data.eq(SRC_PAD_IDX).unsqueeze(1)  # b_size x 1 x len_k\n",
        "  return pad_attn_mask.expand(b_size, len_q, len_k)  # b_size x len_q x len_k #Every sentence --> seq_len x seq_len --> why bec needed to be same dimension when dot(QK)\n",
        " "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2tmOb1x-hQQ"
      },
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(MultiHeadAttention, self).__init__()\n",
        "    self.W_Q = nn.Linear(d_model, d_k*n_heads) #to get query :: each word :embed_dim/d_model --> dk >>for one head\n",
        "    self.W_K = nn.Linear(d_model, d_k*n_heads)\n",
        "    self.W_V = nn.Linear(d_model, d_v*n_heads)\n",
        "    self.linear = nn.Linear(n_heads * d_v, d_model)   #this might not be required\n",
        "    # self.layer_norm = nn.LayerNorm(d_model)\n",
        "\n",
        "  def forward(self,Q,K,V,mask):\n",
        "    #q:[batch, seq_len, embed_dim] , len_q=seq_len=len_k\n",
        "    residual, batch_size = Q, Q.size(0)\n",
        "    \n",
        "    q_s = self.W_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size x n_heads x len_q x d_k]\n",
        "    '''\n",
        "    self.W_Q(Q) : [batch,seq_len, dk*n_heads]\n",
        "    .view(): [batch,seq_len, n_heads, dk]\n",
        "    .transpose(): [batch,n_heads, seq_len,dk]\n",
        "    STAGE 1 of getting every word as dk --> Q of every word\n",
        "    '''\n",
        "\n",
        "    k_s = self.W_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size x n_heads x len_k x d_k] \n",
        "    v_s = self.W_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size x n_heads x len_k x d_v]\n",
        "\n",
        "    #for every head, we have Q,K,V for all words i.e the len_k/len_q are same?\n",
        "    mask = mask.unsqueeze(1).repeat(1, n_heads, 1, 1) # attn_mask : [batch_size x n_heads x len_q x len_k]\n",
        "    #whatever was masking earlier is just replicated for nheads for every sentence, remember masking for eavery sentence is of shape:words x words\n",
        "\n",
        "    context, attn_scores = ScaledDotProduct()(q_s, k_s, v_s, mask)\n",
        "    '''\n",
        "    we do scaled dot product \"nheads\" times  -> got Z for every head now,  Z shape is [batch, ]\n",
        "    context: [batch, nheads, seq_len(len_q/len_k), dv], here dv comes from v_s\n",
        "    attn_scores: [batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "\n",
        "    we have nheads*dv = d_model [in vaswani]  --> (AA)\n",
        "    '''\n",
        "    context = context.transpose(1, 2).contiguous().view(batch_size, -1, n_heads * d_v) #using (AA)\n",
        "    #[batch, seq_len, nheads*dv or d_model]\n",
        "\n",
        "    out = self.linear(context)\n",
        "\n",
        "\n",
        "    \n",
        "    return out, attn_scores #[batch, seq_len, output_size]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZsyNOMAcE1ph"
      },
      "source": [
        "class ScaledDotProduct(nn.Module):\n",
        "  \"Implement Attention as in Transformer-Vaswani \"\n",
        "  def __init__(self):\n",
        "    super(ScaledDotProduct, self).__init__()\n",
        "    self.dropout = nn.Dropout(d3)\n",
        "    '''\n",
        "    query: [batch, nheads, seq_len,dk]\n",
        "    key: [batch, nheads, seq_len,dk]\n",
        "    qkT : [batch, nheads, words,words] \n",
        "        i.e for every head: we have attention vector for each word \n",
        "    '''\n",
        "  def forward(self, query, key, value, mask=None):\n",
        "    scores = torch.matmul(query, key.transpose(-2, -1))/np.sqrt(d_k) #its necessary to write -1,-2  \n",
        "    #[batch_size x n_heads x len_q(=len_k) x len_k(=len_q)]\n",
        "    \n",
        "    if mask is not None:\n",
        "      scores.masked_fill_(mask, -1e9)   #mask is same dimension as scores? yes\n",
        "\n",
        "    attn =  nn.Softmax(dim=-1)(scores) #softmax over last dimension - per each word, find softmax of probs over each word \n",
        "    output = torch.matmul(attn, value)\n",
        "    return output,attn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGP_LwGjAcFA"
      },
      "source": [
        "#http://nlp.seas.harvard.edu/2018/04/03/attention.html#positional-encoding \n",
        "class PositionalEncoding(nn.Module):\n",
        "    \"Implement the PE function.\"\n",
        "    def __init__(self, d_model, max_len=1000):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(d4)\n",
        "        \n",
        "        # Compute the positional encodings once in log space.\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len).unsqueeze(1) #1-D tensor containing 0,1,2,....,max_len\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2) *\n",
        "                             -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe) \n",
        "        #register a buffer that should not to be considered a model parameter,\n",
        "        #saved as part of state_dict along with model params but is not learned\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = x + Variable(self.pe[:, :x.size(1)], \n",
        "                         requires_grad=False)\n",
        "        #print(x.shape)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XaauKQWIZpTs"
      },
      "source": [
        "def clip_gradient(model, clip_value):\n",
        "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
        "    for p in params:\n",
        "        p.grad.data.clamp_(-clip_value, clip_value)\n",
        "    \n",
        "def training(model, iterator, criterion, batch_size,optimizer,clip):\n",
        "  # print(\"training \")\n",
        "  epoch_loss = 0.0\n",
        "  epoch_accuracy = 0.0\n",
        "  #optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters())) # some of the parameters you give the optimizer do not require gradients, and so he don’t know how to handle them\n",
        "  \n",
        "  model.train()\n",
        "  for batch in iterator:\n",
        "    texts,text_lengths = batch.text \n",
        "    target = batch.label.type(torch.LongTensor)\n",
        "\n",
        "    if  texts.shape[0]!=batch_size:\n",
        "      continue;\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "      texts = texts.cuda()\n",
        "      target = target.cuda()    \n",
        "\n",
        "    optimizer.zero_grad() \n",
        "    \n",
        "    predictions_batch  =  model(texts) #[batch, output_size]\n",
        " \n",
        "    loss = criterion(predictions_batch, target)\n",
        "    num_corrects = (torch.max(predictions_batch, 1)[1].view(target.size()).data == target.data).float().sum()\n",
        "    acc = 100.0 * num_corrects/len(batch)\n",
        "\n",
        "    loss.backward()    \n",
        "    clip_gradient(model, clip)   \n",
        "    \n",
        "    optimizer.step()   \n",
        "\n",
        "    epoch_loss += loss.item()  \n",
        "    epoch_accuracy += acc.item()     \n",
        "    \n",
        "  return epoch_loss / len(iterator), epoch_accuracy / len(iterator)  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MlMlv-StZuJn"
      },
      "source": [
        "def evaluation(model, iterator, batch_size):\n",
        "    total_epoch_loss = 0\n",
        "    total_epoch_acc = 0\n",
        "\n",
        "    #deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    #deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            text, text_lengths = batch.text\n",
        "\n",
        "            target = batch.label.type(torch.LongTensor)\n",
        "            if  text.shape[0]!=batch_size:\n",
        "              continue;\n",
        "            if torch.cuda.is_available():\n",
        "              text = text.cuda()\n",
        "              target = target.cuda()    \n",
        "\n",
        "            predictions = model(text)\n",
        "            loss = criterion(predictions, target)\n",
        "            #acc = binary_accuracy(predictions, batch.label)\n",
        "            num_corrects = (torch.max(predictions, 1)[1].view(target.size()).data == target.data).sum()\n",
        "            acc = 100.0 * num_corrects/len(batch)\n",
        "            total_epoch_loss += loss.item()\n",
        "            total_epoch_acc += acc.item()\n",
        "            \n",
        "    return total_epoch_loss/len(iterator), total_epoch_acc/len(iterator)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9o3EKv4papY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "outputId": "a7aa697a-4ce2-4840-9231-836f111f82bc"
      },
      "source": [
        "#Exaplanation of Masking - EXPLANATION#1\n",
        "#for dot(QK) --> #words x #words for ONE sentence\n",
        "tt = torch.tensor([\n",
        "                   [2,4], #sent1 - 3words\n",
        "                   [6,1]\n",
        "                  #  [9,1,1]\n",
        "])\n",
        "tt.size() #[3,3]\n",
        "def get_attn_pad_mask(seq_q, seq_k):\n",
        "  assert seq_q.dim() == 2 and seq_k.dim() == 2\n",
        "  b_size, len_q = seq_q.size()\n",
        "  b_size, len_k = seq_k.size()\n",
        "  pad_attn_mask = seq_k.data.eq(1).unsqueeze(1)  # b_size x 1 x len_k\n",
        "  return pad_attn_mask.expand(b_size, len_q, len_k)  # b_size x len_q x len_k\n",
        "\n",
        "mk = get_attn_pad_mask(tt,tt)  \n",
        "print(mk) #[batch, seq_len , seq_len] i.e  [batch, words, words]\n",
        "print(mk.size())\n",
        "'''\n",
        "why should we calculate word attention wrt to padding ?\n",
        "Ex in this, sent2 have last token as PAD\n",
        "then in matrix equivalent to dot(QK)\n",
        "w1,w2 should not have any attention wrt to pad\n",
        "BUT op is:\n",
        "    [[False, False,  True],\n",
        "     [False, False,  True],\n",
        "     [False*, False*,  True]],\n",
        "'''"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[[False, False],\n",
            "         [False, False]],\n",
            "\n",
            "        [[False,  True],\n",
            "         [False,  True]]])\n",
            "torch.Size([2, 2, 2])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nwhy should we calculate word attention wrt to padding ?\\nEx in this, sent2 have last token as PAD\\nthen in matrix equivalent to dot(QK)\\nw1,w2 should not have any attention wrt to pad\\nBUT op is:\\n    [[False, False,  True],\\n     [False, False,  True],\\n     [False*, False*,  True]],\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    }
  ]
}